{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing with Language: Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# make sure that NLTK language resources have been downloaded \n",
    "# (see \"NLTK Introduction\" notebook)\n",
    "\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Word] Frequency Distributions\n",
    "\n",
    "FreqDist is used to encode \"frequency distributions\", which count the number of times that each outcome of an experiment occurs.\n",
    "* In case of text, its frequency distribution will contains counts of all tokens that appear in the text.\n",
    "* Technically: FreqDist() creates a Python object (that holds information about a frequency distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency distribution of text1\n",
    "fdist1 = FreqDist(text1)\n",
    "\n",
    "print(fdist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FreqDist** methods:\n",
    "\n",
    "* freq(sample) - returns the number of times \"sample\" appears in FreqDist\n",
    "* hapaxes() - a list of samples that appear only once\n",
    "* max() - the sample with the maximum number of occurences\n",
    "* plot() - plot a FreqDist chart\n",
    "* pprint() - \"pretty print\" the first items of FreqDist\n",
    "\n",
    "NLTK book: http://www.nltk.org/book/ch01.html#computing-with-language-simple-statistics\n",
    "\n",
    "Full list of methods: http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print frequency distribution (top results)\n",
    "\n",
    "fdist1.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max()\n",
    "\n",
    "fdist1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq()\n",
    "\n",
    "print(\"','  :\", fdist1.freq(\",\"))\n",
    "print(\"whale:\", fdist1.freq(\"whale\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Information about Python dictionaries: \n",
    "* [\"Dictionaries and Structuring Data\"](https://automatetheboringstuff.com/chapter5/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of fdist1.pprint() looks like a Python \"dictionary\"\n",
    "\n",
    "# can we look up its values by a given \"key\"?\n",
    "fdist1[\"whale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 results (not that interesting for text)\n",
    "\n",
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot the distribution\n",
    "fdist1.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common() returs a list -> we can \"slice\" it\n",
    "\n",
    "my_list = fdist1.most_common(100)\n",
    "\n",
    "# results 50 through 59\n",
    "my_list[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least common results (first 10 examples)\n",
    "\n",
    "fdist1.hapaxes()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words can appear both in lowercase and Capitalized\n",
    "\n",
    "Let's fix our FreqDist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to \"lowercase\" the text before passing it to FreqDist\n",
    "#   - see example in https://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\n",
    "\n",
    "fdist2 = FreqDist(word.lower() for word in text1)\n",
    "\n",
    "# we're going through the list of tokens in text,\n",
    "#  - returning (generating) lowercase versions of these tokens\n",
    "#  - and passing the result to FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial:\n",
    "print(fdist1.freq(\"whale\"))\n",
    "print(fdist1.freq(\"Whale\"))\n",
    "print()\n",
    "\n",
    "# fixed:\n",
    "print(fdist2.freq(\"whale\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data: removing stopwords\n",
    "\n",
    "NLTK contains a corpus of *stopwords* - high-frequency words like \"the\", \"to\" and \"also\" - that we may want to filter out of a document before further processing.\n",
    "\n",
    "Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts.\n",
    "\n",
    "https://www.nltk.org/book/ch02#wordlist-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# English stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with text1 in lowercase\n",
    "\n",
    "# return a list  \n",
    "#   containing \"word.lower()\"\n",
    "#     for every item (stored in variable \"word\")\n",
    "#       in resource \"text1\"\n",
    "\n",
    "text = [word.lower() for word in text1]\n",
    "\n",
    "text[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK book: [4.2   Operating on Every Element](https://www.nltk.org/book/ch01#operating-on-every-element)**\n",
    "\n",
    "This *pattern* – doing something (e.g. modifying) with every item in a sequence and returning a list of results – is called Python *list comprehension*:\n",
    "    \n",
    "`result_list = [item.do_something() for item in list]`\n",
    "\n",
    "List comprehensions may also contain conditions (only items matching the condition will be included in the resulting list):\n",
    "\n",
    "`result_list = [item.do_something() for item in list `**`if`**` condition]`\n",
    "\n",
    "It is very useful for filtering and modifying lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can filter either (a) text before calling FreqDist or (b) results of FreqDist.\n",
    "# let's filter before calling FreqDist.\n",
    "\n",
    "# create a set of stopwords (operations with sets are faster that with lists)\n",
    "stop_set = set(stop_words)\n",
    "\n",
    "# filter out stopwords (return only words not in the stoplist)\n",
    "without_stopwords = [word for word in text if word not in stop_set]\n",
    "\n",
    "text[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also filter out tokens that are not text or numbers\n",
    "\n",
    "# Python has a built-in method .isalnum() that determines \n",
    "# if a string only consists of letters or digits:\n",
    "\n",
    "# https://docs.python.org/3/library/stdtypes.html#str.isalnum\n",
    "\n",
    "filtered = [word for word in without_stopwords if word.isalnum()]\n",
    "\n",
    "filtered[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "\n",
    "freq = FreqDist(filtered)\n",
    "\n",
    "freq.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data: finding interesting words\n",
    "\n",
    "NLTK also includes a list of common English words. We can use it to find unusual or mis-spelt words in a text corpus.\n",
    "\n",
    "See also: https://www.nltk.org/book/ch02#code-unusual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = nltk.corpus.words.words()\n",
    "\n",
    "# convert word list to a set (+ convert words to lowercase)\n",
    "word_set = set(word.lower() for word in word_list)\n",
    "\n",
    "# filter out common words\n",
    "uncommon = [word for word in filtered if word not in word_set]\n",
    "\n",
    "uncommon[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "\n",
    "freq = FreqDist(uncommon)\n",
    "\n",
    "freq.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in order to find really uncommon words we may need to clean data further (convert nouns to singular, etc.) or get a larger list of common words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further information\n",
    "\n",
    "[**Introduction to stylometry with Python**](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python) by François Dominic Laramée\n",
    "* uses FreqDist\n",
    "\n",
    "Stylometry is the quantitative study of literary style through computational distant reading methods. It is based on the observation that authors tend to write in relatively consistent, recognizable and unique ways. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
