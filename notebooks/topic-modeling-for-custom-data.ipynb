{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHNrQ3OvLK8e"
      },
      "source": [
        "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
        "\n",
        "Adopted by Valdis Saulespurens from  [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
        "For questions/comments/improvements, email valdis.s.coding at gmail com<br />\n",
        "___\n",
        "\n",
        "# Latent Dirichlet Allocation (LDA) Topic Modeling\n",
        "\n",
        "**Description:**\n",
        "This [notebook](https://docs.constellate.org/key-terms/#jupyter-notebook) demonstrates how to do topic modeling. The following processes are described:\n",
        "\n",
        "* Filtering based on a [stop words list](https://docs.constellate.org/key-terms/#stop-words)\n",
        "* Cleaning the tokens in the dataset\n",
        "* Creating a [gensim dictionary](https://docs.constellate.org/key-terms/#gensim-dictionary)\n",
        "* Creating a [gensim](https://docs.constellate.org/key-terms/#gensim) [bag of words](https://docs.constellate.org/key-terms/#bag-of-words) [corpus](https://docs.constellate.org/key-terms/#corpus)\n",
        "* Computing a topic list using [gensim](https://docs.constellate.org/key-terms/#gensim)\n",
        "* Visualizing the topic list with `pyldavis`\n",
        "\n",
        "**Use Case:** For Researchers (Mostly code without explanation, not ideal for learners)\n",
        "\n",
        "**Difficulty:** Intermediate\n",
        "\n",
        "**Completion time:** 60 minutes\n",
        "\n",
        "**Knowledge Required:** \n",
        "* Python Basics Series ([Start Python Basics I](./python-basics-1.ipynb))\n",
        "\n",
        "**Knowledge Recommended:**\n",
        "* [Exploring Metadata](./metadata.ipynb)\n",
        "* [Working with Dataset Files](./working-with-dataset-files.ipynb)\n",
        "* [Pandas I](./pandas-1.ipynb)\n",
        "* [Creating a Stopwords List](./creating-stopwords-list.ipynb)\n",
        "* A familiarity with [gensim](https://docs.constellate.org/key-terms/#gensim) is helpful but not required.\n",
        "\n",
        "**Data Format:** [JSON Lines (.jsonl)](https://docs.constellate.org/key-terms/#jsonl)\n",
        "\n",
        "**Libraries Used:**\n",
        "\n",
        "* [pandas](https://constellate.org/docs/key-terms/#pandas) to load a preprocessing list\n",
        "* `csv` to load a custom stopwords list\n",
        "* [gensim](https://docs.constellate.org/key-terms/#gensim) to accomplish the topic modeling\n",
        "* [NLTK](https://docs.constellate.org/key-terms/#nltk) to create a stopwords list (if no list is supplied)\n",
        "* `pyldavis` to visualize our topic model\n",
        "\n",
        "**Research Pipeline**\n",
        "1. Build a dataset\n",
        "2. Create a \"Pre-Processing CSV\" with [Exploring Metadata](./exploring-metadata.ipynb) (Optional)\n",
        "3. Create a \"Custom Stopwords List\" with [Creating a Stopwords List](./creating-stopwords-list.ipynb) (Optional)\n",
        "4. Complete the Topic Modeling analysis with this notebook\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDnWvD2vLK8j"
      },
      "source": [
        "## What is Topic Modeling?\n",
        "\n",
        "**Topic modeling** is a **machine learning** technique that attempts to discover groupings of words (called topics) that commonly occur together in a body of texts. The body of texts could be anything from journal articles to newspaper articles to tweets.\n",
        "\n",
        "**Topic modeling** is an unsupervised, clustering technique for text. We give the machine a series of texts that it then attempts to cluster the texts into a given number of topics. There is also a *supervised*, clustering technique called **Topic Classification**, where we supply the machine with examples of pre-labeled topics and then see if the machine can identify them given the examples.\n",
        "\n",
        "**Topic modeling** is usually considered an exploratory technique; it helps us discover new patterns within a set of texts. **Topic Classification**, using labeled data, is intended to be a predictive technique; we want it to find more things like the examples we give it.\n",
        "\n",
        "<font color='red'>Read more</font>\n",
        "\n",
        "* [\"Latent Dirichlet Allocation: Intuition, math, implementation and visualisation with pyLDAvis\" Ioana](https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094) 2020\n",
        "* [\"Latent Dirichlet Allocation\" Blei, Ng, Jordan](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?TB_iframe=true&width=370.8&height=658.8) 2003"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt93KdoELK8k"
      },
      "source": [
        "## Import your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "id": "Cff71WEkLK8l"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas\n",
        "import os\n",
        "import gensim\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/ValRCS/BSSDH_22/raw/main/corpora/lv_old_newspapers_5k.tsv\"\n",
        "\n",
        "df = pandas.read_csv(url, sep=\"\\t\") \n",
        "df.head()"
      ],
      "metadata": {
        "id": "784x8uBHv_p3",
        "outputId": "b2bd2953-a33a-4116-c72c-ea3ca5539ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Language           Source        Date  \\\n",
              "0  Latvian     rekurzeme.lv  2008/09/04   \n",
              "1  Latvian         diena.lv  2012/01/10   \n",
              "2  Latvian  bauskasdzive.lv  2007/12/27   \n",
              "3  Latvian  bauskasdzive.lv  2008/10/08   \n",
              "4  Latvian         diena.lv  2011/10/05   \n",
              "\n",
              "                                                Text  \n",
              "0  \"Viņa pirmsnāves zīmītē bija rakstīts vienīgi ...  \n",
              "1                               info@zurnalistiem.lv  \n",
              "2  Bhuto, kas Pakistānā no trimdas atgriezās tika...  \n",
              "3  Plkst. 4.00 Samoilovs / Pļaviņš (pludmales vol...  \n",
              "4  CVK bija vērsusies Skaburska, lūdzot izskaidro...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b751a5e3-06ac-4754-93b5-6e53cfd94c45\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Language</th>\n",
              "      <th>Source</th>\n",
              "      <th>Date</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Latvian</td>\n",
              "      <td>rekurzeme.lv</td>\n",
              "      <td>2008/09/04</td>\n",
              "      <td>\"Viņa pirmsnāves zīmītē bija rakstīts vienīgi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Latvian</td>\n",
              "      <td>diena.lv</td>\n",
              "      <td>2012/01/10</td>\n",
              "      <td>info@zurnalistiem.lv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Latvian</td>\n",
              "      <td>bauskasdzive.lv</td>\n",
              "      <td>2007/12/27</td>\n",
              "      <td>Bhuto, kas Pakistānā no trimdas atgriezās tika...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Latvian</td>\n",
              "      <td>bauskasdzive.lv</td>\n",
              "      <td>2008/10/08</td>\n",
              "      <td>Plkst. 4.00 Samoilovs / Pļaviņš (pludmales vol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Latvian</td>\n",
              "      <td>diena.lv</td>\n",
              "      <td>2011/10/05</td>\n",
              "      <td>CVK bija vērsusies Skaburska, lūdzot izskaidro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b751a5e3-06ac-4754-93b5-6e53cfd94c45')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b751a5e3-06ac-4754-93b5-6e53cfd94c45 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b751a5e3-06ac-4754-93b5-6e53cfd94c45');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_documents = list(df.Text)\n",
        "len(raw_documents) # we will use each document separately"
      ],
      "metadata": {
        "id": "ZPW079krxDhq",
        "outputId": "151ab201-f799-4098-ea1d-0c87530e1dd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4999"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH4HHrMCLK8m"
      },
      "source": [
        "## Load Stopwords List\n",
        "\n",
        "If you have created a stopword list in the stopwords notebook, we will import it here. (You can always modify the CSV file to add or subtract words then reload the list.) Otherwise, we'll load the NLTK [stopwords](https://docs.constellate.org/key-terms/#stop-words) list automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "scrolled": true,
        "id": "Y6m7PhOlLK8m",
        "outputId": "606490d0-1128-4dec-9f01-0f0682ed8409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(161, ['aiz', 'ap', 'apakš', 'apakšpus', 'ar'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# how to find all languages stopwords built in NLTK\n",
        "# https://stackoverflow.com/questions/54573853/nltk-available-languages-for-stopwords\n",
        "# bigger collection of all stopwords\n",
        "# https://github.com/stopwords-iso\n",
        "# latvian https://github.com/stopwords-iso/stopwords-lv/raw/master/stopwords-lv.txt\n",
        "url = \"https://github.com/stopwords-iso/stopwords-lv/raw/master/stopwords-lv.txt\"\n",
        "stop_words = []\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    stop_words = response.text.split()\n",
        "len(stop_words), stop_words[:5]\n",
        "# see previous session on how to save locally your stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txPYYaLiLK8n"
      },
      "source": [
        "## Define a Function to Process Tokens\n",
        "Next, we create a short function to clean up our tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "id": "vX50-YNsLK8n"
      },
      "outputs": [],
      "source": [
        "def process_token(token):\n",
        "    token = token.lower()\n",
        "    if token in stop_words:\n",
        "        return\n",
        "    if len(token) < 4:\n",
        "        return\n",
        "    if not(token.isalpha()):\n",
        "        return\n",
        "    return token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "id": "XeISd_yfLK8q",
        "outputId": "298d296e-a8fc-437f-ca1b-7593f01401f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted all documents to list of clean tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['pirmsnāves',\n",
              "  'zīmītē',\n",
              "  'rakstīts',\n",
              "  'vienīgi',\n",
              "  'smēķēšanas',\n",
              "  'aizlieguma',\n",
              "  'radītajiem',\n",
              "  'laikrakstam',\n",
              "  'paskaidroja',\n",
              "  'nelaiķa',\n",
              "  'svainis',\n",
              "  'helmuts',\n",
              "  'nebija',\n",
              "  'vērsta',\n",
              "  'viņa',\n",
              "  'ģimeni'],\n",
              " [],\n",
              " ['pakistānā',\n",
              "  'trimdas',\n",
              "  'atgriezās',\n",
              "  'diviem',\n",
              "  'uzstājās',\n",
              "  'priekšvēlēšanu',\n",
              "  'organizēts',\n",
              "  'nākamajā',\n",
              "  'mēnesī',\n",
              "  'gaidāmajām',\n",
              "  'parlamenta']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# %%time\n",
        "# Limit to n documents. Set to None to use all documents.\n",
        "\n",
        "limit = None\n",
        "\n",
        "n = 0\n",
        "documents = []\n",
        "for document in raw_documents:\n",
        "    tokens = document.split()\n",
        "    processed_document = [process_token(token) for token in tokens if process_token(token) is not None] # TODO could be improved with new walrus :=\n",
        "    documents.append(processed_document)\n",
        "print(f'Converted all documents to list of clean tokens')\n",
        "documents[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr5Nz3cYLK8r"
      },
      "source": [
        "Build a gensim dictionary corpus and then train the model. More information about parameters can be found at the [Gensim LDA Model page](https://radimrehurek.com/gensim/models/ldamodel.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "id": "dYPa00rRLK8r"
      },
      "outputs": [],
      "source": [
        "dictionary = gensim.corpora.Dictionary(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": false,
        "id": "okazgqZPLK8s"
      },
      "outputs": [],
      "source": [
        "doc_count = len(documents)\n",
        "num_topics = 7 # Change the number of topics\n",
        "passes = 5 # The number of passes used to train the model\n",
        "# Remove terms that appear in less than 50 documents and terms that occur in more than 90% of documents.\n",
        "dictionary.filter_extremes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "id": "e6ouGd8NLK8t"
      },
      "outputs": [],
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3ci01tpKLK8t"
      },
      "outputs": [],
      "source": [
        "bow_corpus = []\n",
        "for doc in documents:\n",
        "    bow = dictionary.doc2bow(doc)\n",
        "    bow_corpus.append(bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "id": "iA7Kquv-LK8u",
        "outputId": "969060a9-314d-4ee5-e315-33bdafb8776b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.69 s, sys: 121 ms, total: 8.81 s\n",
            "Wall time: 8.85 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Train the LDA model\n",
        "model = gensim.models.LdaModel(\n",
        "    corpus=bow_corpus,\n",
        "    id2word=dictionary,\n",
        "    num_topics=num_topics,\n",
        "    passes=passes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIIAIEFJLK8u"
      },
      "source": [
        "## Perplexity\n",
        "\n",
        "After each pass, the LDA model will output a \"perplexity\" score that measures the \"held out log-likelihood\". Perplexity is a measure of how \"surpised\" the machine is to see certain data. In other words, perplexity measures how successfully a trained topic model predicts new data. The model may be trained many times with different parameters, optimizing for the lowest possible perplexity.\n",
        "\n",
        "In general, the perplexity score should trend downward as the machine \"learns\" what to expect from the data. While a low perplexity score may signal the machine has learned the documents' patterns, that does not mean that the topics formed from a model with low perplexity will form the most coherent topics. (See [\"Reading Tea Leaves: How Humans Interpret Topic Models\" Chang, et al. 2009](https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html).)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNZTbhlTLK8v"
      },
      "source": [
        "## Topic Coherence\n",
        "\n",
        "The failure of perplexity scores to consistently create \"good\" topics has led to new methods in \"topic coherence\". Here we demonstrate two of these methods with Gensim but there are additional methods available. Ideally, a researcher would run many topic models, discovering the optimum settings for topic coherence.\n",
        "\n",
        "Ultimately, however, the best judgment of topic coherence is a disciplinary expert, particularly someone with familiarity with the materials in question.\n",
        "\n",
        "<font color='red'>Read more</font>\n",
        "\n",
        "* [\"Optimizing Semantic Coherence in Topic Models\" Mimno, et al. 2011](http://dirichlet.net/pdf/mimno11optimizing.pdf)\n",
        "* [\"Automatic Evaluation of Topic Coherence\" Newman, et al. 2010](https://mimno.infosci.cornell.edu/info6150/readings/N10-1012.pdf))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GjmmKTGpLK8v",
        "outputId": "75432e74-4556-48b3-c488-0d129c0fcb48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Coherence Score:  -5.043044180565978\n"
          ]
        }
      ],
      "source": [
        "# Compute the coherence score using UMass\n",
        "# u_mass is measured from -14 to 14, higher is better\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "coherence_model_lda = CoherenceModel(\n",
        "    model=model,\n",
        "    corpus=bow_corpus,\n",
        "    dictionary=dictionary, \n",
        "    coherence='u_mass'\n",
        ")\n",
        "\n",
        "# Compute Coherence Score using UMass\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLW4IdaoLK8w"
      },
      "source": [
        "## Display a List of Topics\n",
        "Print the most significant terms, as determined by the model, for each topic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": true,
        "id": "dzESr9QCLK8x",
        "outputId": "964b7079-850a-4ff2-d801-56f2d2f16684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0     gadā gadus viņš policijas valsts automašīnu darba mums ļoti visu\n",
            "Topic 1     latvijas mākslas jānis gada savu darbu bijis domes pulksten divas\n",
            "Topic 2     latvijas eiropas valsts varētu viņš cenu laikā rīgas latvijā tirgus\n",
            "Topic 3     gada šogad latvijas gadā latu vairāk trīs aptuveni nodokļa kopumā\n",
            "Topic 4     valsts latvijas latviešu darba reizi valodas paši iespējams pirmais būtu\n",
            "Topic 5     ļoti viņa būtu laikā viņš mūsu viņi daudz mums varētu\n",
            "Topic 6     latvijas novada izglītības darba valsts pagasta finanšu pašvaldības domes bērnu\n"
          ]
        }
      ],
      "source": [
        "for topic_num in range(0, num_topics):\n",
        "    word_ids = model.get_topic_terms(topic_num)\n",
        "    words = []\n",
        "    for wid, weight in word_ids:\n",
        "        word = dictionary.id2token[wid]\n",
        "        words.append(word)\n",
        "    print(\"Topic {}\".format(str(topic_num).ljust(5)), \" \".join(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYT-uAYdLK8x"
      },
      "source": [
        "## Visualize the Topic Distances\n",
        "\n",
        "Visualize the model using [`pyLDAvis`](https://pyldavis.readthedocs.io/en/latest/). This visualization can take a while to generate depending on the size of your dataset.\n",
        "\n",
        "Try choosing a topic and adjusting the λ slider. When λ approaches 0, the words in a given document occur almost entirely in that topic. When λ approaches 1, the words occur more often in other topics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# most likely we do not have pyLDAvis visualization library so we will install it\n",
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "UDJLYKsPyqvi",
        "outputId": "a4291dac-28cf-4942-df03-1d713ee8bcc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 9.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.8.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.7.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136898 sha256=e769aba1fc02b79c4e9e59470ef59914ef76624635e9f98f43cced8cb27d8e06\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: funcy, pyLDAvis\n",
            "Successfully installed funcy-1.17 pyLDAvis-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# later versions pyLDAvis do not play well with colab\n",
        "# https://stackoverflow.com/questions/66096149/pyldavis-visualization-from-gensim-not-displaying-the-result-in-google-colab"
      ],
      "metadata": {
        "id": "nD4tS4EB0Tuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "id": "arSk_QbjLK8y",
        "outputId": "78ffe797-4ef3-4b3d-f300-e1db7da17a52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        }
      ],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# feed the LDA model into the pyLDAvis instance\n",
        "lda_viz = gensimvis.prepare(model, bow_corpus, dictionary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "scrolled": true,
        "id": "738u2GbILK8y",
        "outputId": "dd5f6f69-12bd-408f-d87f-2a3f4ba93911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        }
      ],
      "source": [
        "# Export this visualization as an HTML file\n",
        "# An internet connection is still required to view the HTML\n",
        "p = gensimvis.prepare(model, bow_corpus, dictionary)\n",
        "pyLDAvis.save_html(p, 'my_visualization.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare a topic model on your own corpus"
      ],
      "metadata": {
        "id": "85lIStCPNgKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code goes here"
      ],
      "metadata": {
        "id": "uSoKnbDtNdPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Submit Assignment](https://forms.gle/cbBP4LVXNbdMFtfZ8)\n",
        "\n",
        "Note: requires gmail account, if you do not have one, you can email submission directly to valdis.s.coding at gmail com"
      ],
      "metadata": {
        "id": "cgySDYuhNkaj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "topic-modeling-for-custom-data.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}