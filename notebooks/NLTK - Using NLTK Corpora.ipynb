{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Corpora and Lexical Resources\n",
    "\n",
    "based on the NLTK book:\n",
    "\n",
    "[\"Accessing Text Corpora and Lexical Resources\"](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Text Corpora\n",
    "\n",
    "NLTK includes many text collections (corpora) and other language resources, listed here: http://www.nltk.org/nltk_data/\n",
    "\n",
    "Additional information:\n",
    "* [NLTK Corpus How-to](http://www.nltk.org/howto/corpus.html)\n",
    "\n",
    "In order to use these resources you may need to download them using `nltk.download()`\n",
    "\n",
    "---\n",
    "\n",
    "**NLTK book: [\"Text Corpus Structure\"](https://www.nltk.org/book/ch02#text-corpus-structure)**\n",
    "\n",
    "There are different types of corpora:\n",
    "* simple collections of text (e.g. Gutenberg corpus)\n",
    "* categorized (texts are grouped into categories that might correspond to genre, source, author)\n",
    "* temporal, demonstrating language use over a time period (e.g. news texts)\n",
    "\n",
    "![Types of NLTK corpora](https://www.nltk.org/images/text-corpus-structure.png)\n",
    "\n",
    "There are also annotated text corpora that contain linguistic annotations, representing POS tags, named entities, semantic roles, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Gutenberg Corpus\n",
    "\n",
    "NLTK includes a small selection of texts (= multiple files) from the Project Gutenberg electronic text archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore its contents:\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Emma\" by Jane Austen\n",
    "\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "\n",
    "print(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can access corpus texts as characters, words (tokens) or sentences:\n",
    "\n",
    "file_id = 'austen-emma.txt'\n",
    "\n",
    "print(\"\\nSentences:\")\n",
    "print( nltk.corpus.gutenberg.sents(file_id)[:3] )\n",
    "\n",
    "print(\"\\nWords:\")\n",
    "print( nltk.corpus.gutenberg.words(file_id)[:10] )\n",
    "\n",
    "print(\"\\nChars:\")\n",
    "print( nltk.corpus.gutenberg.raw(file_id)[:50] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.nltk.org/book/ch02#gutenberg-corpus on how to compute statistics of words, sentences and characters (e.g. avg words per sentence).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Brown corpus\n",
    "\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University.\n",
    "\n",
    "This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown corpus categories list:\n",
    "\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter the corpus by (a) one or more categories or (b) file IDs:\n",
    "\n",
    "print(brown.sents(categories='science_fiction')[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words(fileids=['cg22']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use NLTK **ConditionalFreqDist** to collect statistics on the corpus distribution across genres and other properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brown corpus contains tags with part-of-speech information\n",
    "\n",
    "[Working with Tagged Corpora](https://www.nltk.org/book/ch05#tagged-corpora) (NLTK book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.corpus.brown.tagged_words(tagset='universal')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# islice() lets us read a part of the corpus\n",
    "\n",
    "from itertools import islice\n",
    "words = islice(words, 300)\n",
    "\n",
    "# let's convert it to a list\n",
    "word_list = list(words)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all words with POS tag \"ADJ\"\n",
    "\n",
    "tag = 'ADJ'\n",
    "\n",
    "[item[0] for item in word_list if item[1] == tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional examples** (using FreqDist, ...):\n",
    "    \n",
    "[Working with Tagged Corpora](https://www.nltk.org/book/ch05#tagged-corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) NLTK Corpus functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fileids()  = the files of the corpus\n",
    "* fileids([categories])  = the files of the corpus corresponding to these categories\n",
    "\n",
    "* categories()  = the categories of the corpus\n",
    "* categories([fileids])  = the categories of the corpus corresponding to these files\n",
    "\n",
    "* raw()  = the raw content of the corpus\n",
    "* raw(fileids=[f1,f2,f3])  = the raw content of the specified files\n",
    "* raw(categories=[c1,c2])  = the raw content of the specified categories\n",
    "\n",
    "* words()  = the words of the whole corpus\n",
    "* words(fileids=[f1,f2,f3])  = the words of the specified fileids\n",
    "* words(categories=[c1,c2])  = the words of the specified categories\n",
    "\n",
    "* sents()  = the sentences of the whole corpus\n",
    "* sents(fileids=[f1,f2,f3])  = the sentences of the specified fileids\n",
    "* sents(categories=[c1,c2])  = the sentences of the specified categories\n",
    "\n",
    "* abspath(fileid)  = the location of the given file on disk\n",
    "* encoding(fileid)  = the encoding of the file (if known)\n",
    "* open(fileid)  = open a stream for reading the given corpus file\n",
    "* root  = if the path to the root of locally installed corpus\n",
    "\n",
    "* readme()  = the contents of the README file of the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: if you want to explore these corpora using `nltk.Text` functionality (e.g. as in the Introduction part) you will need to load them into `nltk.Text`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "\n",
    "Choose one of NLTK corpora and **explore it using NLTK** (following examples here and in the NLTK book).\n",
    "\n",
    "Also apply what you learned (FreqDist, ...) in section \"Computing with Language: Statistics\".\n",
    "\n",
    "---\n",
    "\n",
    "**Write code in notebook cells below**.\n",
    "* add more cells (use \"+\" icon) if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Resources\n",
    "\n",
    "A lexicon, or lexical resource, is a collection of words and/or phrases along with associated information such as part of speech and sense definitions.\n",
    "\n",
    "https://www.nltk.org/book/ch02#lexical-resources\n",
    "\n",
    "We already used NLTK lexical resources (stopwords and common English words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a collection of synonym sets related to \"wind\"\n",
    "\n",
    "wn.synsets('wind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words (lemmas) in one of synsets:\n",
    "\n",
    "wn.synset('wind.n.08').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('wind.n.08').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('wind.n.08').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore all the synsets for this word\n",
    "\n",
    "for synset in wn.synsets('wind'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all synsets that contain a given word\n",
    "\n",
    "wn.lemmas('curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Additional WordNet examples:**\n",
    "* https://www.nltk.org/book/ch02#wordnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
